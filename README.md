# AI for Finance Hackathon — Corporate Knowledge Assistant

## Что это за проект
Этот репозиторий содержит решение для хакатона **AI for Finance**, где мы собрали цифрового ассистента для сотрудников корпоративных клиентов. Ассистент отвечает на вопросы про внутренние регламенты, помогает найти документы и формирует ответы на основе актуального контента, объединяя Retrieval-Augmented Generation (RAG) и долговременную память LLM. В итоге сотрудник получает релевантные выдержки из базы знаний и готовые документы прямо в чате, не переключаясь между сервисами.

## Ключевые возможности
- **Умный поиск по внутренним материалам.** RAG-пайплайн индексирует PDF, DOCX, HTML, изображения и текстовые файлы, извлекает текст с помощью OCR и сохраняет эмбеддинги в Qdrant, чтобы ответы основывались на фактических данных компании.
- **Персонализация и долговременная память.** Бэкенд хранит компактный профайл пользователя и накопленную переписку, агрегируя её в `llm_memory`, поэтому ассистент помнит предпочтения сотрудника, но не превышает лимиты БД.
- #ToDo **Голосовой и текстовый режимы.** Фронтенд предоставляет привычное чат-окно с переключателем голосового ввода; бэкенд умеет конвертировать текст в аудио, чтобы сценарий работал и для hands-free.
- **Управление документами и правами.** REST API позволяет загружать документы, настраивать доступы и контролировать выдачу через DocCall/DocPermission контроллеры.
- **Прозрачная архитектура.** Проект разделён на независимые сервисы (Flask API, FastAPI RAG, React-клиент), все они поднимаются через Docker Compose и могут развёртываться выборочно.

## Структура репозитория
| Путь | Назначение                                                                                                |
| --- |-----------------------------------------------------------------------------------------------------------|
| `api/` | Flask-приложение с контроллерами для пользователей, документов, чатов, аудио и долговременной памяти LLM. |
| `client/` | Веб-клиент на #ToDo с чат-интерфейсом и голосовым переключателем.                                         |
| `rag_pipeline/` | FastAPI-сервис, который индексирует документы в Qdrant и отвечает на поисковые запросы.                   |
| `documents/` | Локальная директория для исходных файлов, которые нужно проиндексировать.                                 |
| `docker-compose.yml` | Инфраструктура (Flask API, Postgres, Qdrant, RAG-сервис) для локального запуска.                          |

## Архитектура
1. **Flask API** (`api/app.py`) обрабатывает HTTP-запросы клиентов, хранит данные в Postgres и проксирует обращения к LLM.
2. **RAG-сервис** (`rag_pipeline/main.py`) периодически подтягивает список документов из API, парсит их, создаёт эмбеддинги через Sentence Transformers и сохраняет в Qdrant.
3. **LLM-память** (`api/utils/memory_utils.py`) добавляет к каждой сессии историю и профиль пользователя, автоматически суммаризирует длинные цепочки, чтобы не превысить лимиты.
4. **React-клиент** #ToDo даёт сотруднику единое окно общения, отображает найденные данные и поддерживает targeted actions.
5. **Документооборот** (`api/Controllers/DocumentController.py`, `DocCallController.py`, `DocPermissionController.py`) гарантирует, что ассистент учитывает права доступа и может вернуть либо ссылку на файл, либо сам документ.

## Бэкенд (Flask)
- **MVC-организация.** Контроллеры в `api/Controllers` делят ответственность по доменам: пользователи и роли, чаты, сообщения, документы, разрешения и DocCall, а также вспомогательные маршруты для аудио и памяти.
- **Аутентификация и управление пользователями.** Через `/api/users`, `/api/managers`, `/api/employees` создаём профили, далее JWT-эндпоинты в Auth контроллере выдают токены.
- **Чат и память.** `MessageController` сохраняет историю сообщений, обращается к OpenRouter/LLM и дополняет ответы RAG-контекстом и долговременной памятью из `llm_memory`.
- #ToDo **Аудиосервис.** `/api/converttexttoaudio/` конвертирует текстовые ответы в аудио-файлы, чтобы голосовой ассистент звучал естественно.

## Retrieval-Augmented Generation
- **Индексирование:** `rag_pipeline/main.py` читает метаданные документов через `/api/documents`, скачивает или открывает файлы, очищает текст, режет на чанки и строит эмбеддинги моделью `all-MiniLM-L6-v2` (по умолчанию).
- **Поддержка разных форматов:** встроенные парсеры для TXT/CSV/JSON, HTML, DOCX, PDF, изображений; если PDF не содержит текста, он прогоняется через PaddleOCR.
- **Хранилище эмбеддингов:** Qdrant (`COLLECTION_NAME=documents_rag`) хранит чанки с payload`ом (id документа, номер фрагмента, исходное название).
- **Поиск:** FastAPI предоставляет `/search` для похожих запросов и `/build` для переиндексации (с параметром `reindex_existing`).

## Фронтенд #ToDo

## Запуск
1. **Подготовка окружения**
   - Установите Docker и Docker Compose.
   - Создайте `.env` в корне (см. переменные из `docker-compose.yml` и сервисов).
2. **Старт сервисов**
   ```bash
   docker-compose up --build
   ```
   Это поднимет Flask API (`:5000`), Postgres, Qdrant и RAG FastAPI (`:8000`).
3. **Индексация документов**
   - Скопируйте файлы в `documents/` или укажите `url`/`path` у записей `/api/documents`.
   - Вызовите RAG-сервис:
     ```bash
     curl -X POST http://localhost:8000/build
     ```
   - Проверить выдачу можно через:
     ```bash
     curl -X POST http://localhost:8000/search -H "Content-Type: application/json" -d '{"question": "Политика по командировкам"}'
     ```
4. **Запуск фронтенда** #ToDo

   Приложение подключится к локальному API и позволит вести диалог с ассистентом.

## Типичный сценарий работы
1. Сотрудник авторизуется в веб-клиенте и открывает чат.
2. Сообщение уходит в Flask API, который подтягивает профиль пользователя, последние сообщения и долговременную память.
3. API запрашивает RAG-сервис для релевантных фрагментов документов и передаёт их вместе с памятью в LLM.
4. Ответ ассистента сохраняется в истории, краткая выжимка попадает в `llm_memory`, а при необходимости генерируется аудио.
5. Если в ответе нужно выдать документ, DocCall проверяет права пользователя и прикладывает нужные файлы.